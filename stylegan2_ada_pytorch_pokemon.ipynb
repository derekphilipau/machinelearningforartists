{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylegan2-ada-pytorch-pokemon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMzPUZdJDJwrnw9OHosJlk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derekphilipau/machinelearningforartists/blob/main/stylegan2_ada_pytorch_pokemon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba1NQzpO9iJ4"
      },
      "source": [
        "# Training Pokemon with StyleGAN\n",
        "\n",
        "In this tutorial, we only want to familiarize ourselves with running StyleGAN (in this case, the newest version) and seeing how training progresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLqHshr2y-MO"
      },
      "source": [
        "## Verify Runtime is GPU\n",
        "\n",
        "In the menu, select Runtime -> Change Runtime Type and verify you are using the **GPU**.  Also select **High-RAM** if you are using Colab Pro.\n",
        "\n",
        "The `nvidia-smi` command below should **NOT** display *\"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JvPMLWy95f",
        "outputId": "4b5f0461-8946-444b-be7c-0aaf4e8e70ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-cf5debe3-1e8f-93bd-8005-fce579ab2073)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUl3Rt0ryX7m"
      },
      "source": [
        "## Mount your Google Drive\n",
        "\n",
        "You will be storing the training models and progress images on your Google Drive.  This is very convenient for viewing progress, and if your Colab notebook is disconnected you will not lose your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag2Bb1pPzthT",
        "outputId": "f51b9412-0f93-4f8a-8306-a6d8ef27e0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIHuj2eEy13f"
      },
      "source": [
        "## Install Stylegan2-ada-pytorch Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8r0Ca7Hpo5F"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y195-nOz19D"
      },
      "source": [
        "Verify the next command results in \"1\".  If not, go back to the beginning and verify you have a GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV07xnKTprut",
        "outputId": "0ccabe15-8131-40e1-f85d-4fbee5f590a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jY44x99pusN"
      },
      "source": [
        "import torchvision"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZhoCdj4pREI",
        "outputId": "42689912-6b21-4c24-ab80-caa0e00d2b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pyspng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/b2/c18f96ccc62153631fdb3122cb1e13fa0c89303f4e64388a49a04bfad9f2/pyspng-0.1.0-cp37-cp37m-manylinux2010_x86_64.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 9.0MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 15.9MB/s \n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/0f/4b49476d185a273163fa648eaf1e7d4190661d1bbf37ec2975b84df9de02/imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyspng) (1.19.5)\n",
            "Installing collected packages: pyspng, ninja, imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.3 ninja-1.10.0.post2 pyspng-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94le1Kh_v1k"
      },
      "source": [
        "## Get the StyleGAN code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "najHDP7gMeFy",
        "outputId": "94f97db9-efd4-4ab7-ed32-4a89096d7b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Total 120 (delta 0), reused 0 (delta 0), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (120/120), 1.12 MiB | 27.33 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCYmJ2Z60WSe"
      },
      "source": [
        "mkdir /content/stylegan2-ada-pytorch/datasets"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OscsNQnMiqa",
        "outputId": "14234538-3222-4a2d-ec73-8bca1e62defb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd /content/stylegan2-ada-pytorch/datasets"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/stylegan2-ada-pytorch/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpk9vGb50lee"
      },
      "source": [
        "## Get Pokemon dataset of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyv63QC7xc6l"
      },
      "source": [
        "!wget https://github.com/derekphilipau/machinelearningforartists/raw/main/docs/src/week2/pokemon256.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSNoTfiPcrNy"
      },
      "source": [
        "!unzip pokemon256.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anm9DkOccyMZ"
      },
      "source": [
        "!rm -rf __MACOSX"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsBFXqK1NPGl"
      },
      "source": [
        "!rm pokemon256.zip"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBg2Zy06dkqz"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQfprRHj_-gH"
      },
      "source": [
        "## Prepare Pokemon dataset for use by StyleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_szXOaO0WI00"
      },
      "source": [
        "!python dataset_tool.py --source=./datasets/pokemon256 --dest=./datasets/pokemondataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTo4K3m8AEzh"
      },
      "source": [
        "## Create Folders on Your Google Drive\n",
        "\n",
        "If we accidentally close our browser or the Colab runtime disconnects, we will lose all of our training models and progress images.  Therefore we want to store the training data on our Google Drive.  The following cells will create a new folder on your Google Drive, **MachineLearningForArtists**.  The Pokemon training data will be stored in **MachineLearningForArtists/Pokemon**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MexWxMvLHP5O"
      },
      "source": [
        "mkdir /content/drive/MyDrive/MachineLearningForArtists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tja3AaYIHU9v"
      },
      "source": [
        "mkdir /content/drive/MyDrive/MachineLearningForArtists/Pokemon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpR6A_qBAo8m"
      },
      "source": [
        "## Train from Scratch\n",
        "\n",
        "This cell block will train StyleGAN from scratch.  Training from scratch is much slower than using *transfer learning* on a previously trained model.  However, the purpose of this tutorial is to a) try training for the first time and b) notice how the progress images develop over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuSkETl1V8pv"
      },
      "source": [
        "!python train.py --outdir=/content/drive/MyDrive/MachineLearningForArtists/Pokemon --data=./datasets/pokemondataset.zip --gpus=1 --augpipe=bg --gamma=10 --cfg=paper256 --mirror=1 --snap=10 --metrics=none\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQVRH5rr4dki"
      },
      "source": [
        "## Wait an Hour or Two\n",
        "\n",
        "Normal training of a dataset can take days.  For this demonstration, we only want to train for a few hours in order to get familiar with the training process and see the initial results.\n",
        "\n",
        "Results are stored in your Google Drive under drive/MyDrive/MachineLearningForArtists/Pokemon\n",
        "\n",
        "Each training run is stored in a separate directory.  The initial training run is stored in `00000-pokemondataset...`.  If you run training twice, the second run will be stored in `00001-pokemondataset...`.  And so on.\n",
        "\n",
        "Inside the training run directory you will see various files.  `reals.png` shows a sample of the training dataset.  You should see various Pokemon in this image. `fakes000000.png` is a sample of generated images from the initial model.  `network-snapshot-XXXXXX.pkl` is the actual model which can be used later to generate \"fake\" images, videos, etc.\n",
        "\n",
        "As training progresses you will see more `fakes` & `network-snapshot` files created.\n",
        "\n",
        "Since we only want to test a training, once you get to around the 500th iteration, you can stop the above training cell.  Then take a look at the `fakes` images and familiarize yourself with how the training progresses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ-xirKuHKEP"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Now, let's train the same dataset but use *transfer learning*.  We will use a StyleGAN2 model already pre-trained on Pokemon to train a new model that is trained against a Tamagotchi dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM9mywxwPy8k"
      },
      "source": [
        "### Get the Tomagotchi dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm3cBBuTO4rC"
      },
      "source": [
        "%cd datasets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68IvkgSOqwO"
      },
      "source": [
        "!wget https://github.com/derekphilipau/machinelearningforartists/raw/main/docs/src/week2/Tamagotchi256.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCQfcO9mO-3d"
      },
      "source": [
        "!unzip Tamagotchi256.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lNNF3pcPCwM"
      },
      "source": [
        "!rm -rf __MACOSX/"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrHIFInOPGi-",
        "outputId": "abaf7559-caaa-4c19-db31-8e03257a1a88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/stylegan2-ada-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fa3gVmTPuXk"
      },
      "source": [
        "### Prepare Tomagotchi dataset for use by StyleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GvnuNjSPLC5"
      },
      "source": [
        "!python dataset_tool.py --source=./datasets/Tamagotchi256 --dest=./datasets/tamagotchidataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XCXysDmOnqF"
      },
      "source": [
        "### Get the pre-trained Pokemon model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN9c3nyQJWi-"
      },
      "source": [
        "!gdown --id 122_Nu7p6ESMqBarxuAHCXhQuAaNCsh_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbn23OUQIS7"
      },
      "source": [
        "### Train using the Pokemon model with the Tamagotchi dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIqAPSgaHKEZ",
        "outputId": "2c5e9609-0903-486b-fbc2-f2218c71af50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train.py --resume=./stylegan2-ada-pytorch-pokemon-256px-11650.pkl --outdir=/content/drive/MyDrive/MachineLearningForArtists/Tamagotchi --data=./datasets/tamagotchidataset.zip --gpus=1 --augpipe=bg --gamma=10 --cfg=paper256 --mirror=1 --snap=10 --metrics=none\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"./datasets/tamagotchidataset.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 725,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 8\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 10.0\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 64,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 20,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"./stylegan2-ada-pytorch-pokemon-256px-11650.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/MachineLearningForArtists/Tamagotchi/00000-tamagotchidataset-mirror-paper256-gamma10-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/MachineLearningForArtists/Tamagotchi/00000-tamagotchidataset-mirror-paper256-gamma10-bg-resumecustom\n",
            "Training data:      ./datasets/tamagotchidataset.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   725\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  1450\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"./stylegan2-ada-pytorch-pokemon-256px-11650.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           262656      -        [8, 512]            float32 \n",
            "mapping.fc1           262656      -        [8, 512]            float32 \n",
            "mapping.fc2           262656      -        [8, 512]            float32 \n",
            "mapping.fc3           262656      -        [8, 512]            float32 \n",
            "mapping.fc4           262656      -        [8, 512]            float32 \n",
            "mapping.fc5           262656      -        [8, 512]            float32 \n",
            "mapping.fc6           262656      -        [8, 512]            float32 \n",
            "mapping.fc7           262656      -        [8, 512]            float32 \n",
            "mapping               -           512      [8, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [8, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [8, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [8, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [8, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [8, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [8, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [8, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [8, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [8, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [8, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [8, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [8, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [8, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [8, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [8, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [8, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [8, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 24767458    175568   -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   256         16       [8, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [8, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [8, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [8, 128, 128, 128]  float16 \n",
            "b256           -           16       [8, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [8, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [8, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [8, 256, 64, 64]    float16 \n",
            "b128           -           16       [8, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [8, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [8, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [8, 512, 32, 32]    float16 \n",
            "b64            -           16       [8, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [8, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [8, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [8, 512, 16, 16]    float16 \n",
            "b32            -           16       [8, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [8, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n",
            "b16            -           16       [8, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [8, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n",
            "b8             -           16       [8, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [8, 512]            float32 \n",
            "b4.out         513         -        [8, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          24001089    416      -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2021-03-18 02:51:52.590482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      time 1m 00s       sec/tick 14.8    sec/kimg 231.32  maintenance 45.3   cpumem 5.00   gpumem 10.66  augment 0.000\n",
            "\n",
            "Aborted!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}