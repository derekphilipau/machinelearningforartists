(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{393:function(a,t,e){"use strict";e.r(t);var s=e(40),r=Object(s.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"week-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#week-3"}},[a._v("#")]),a._v(" Week 3")]),a._v(" "),e("h2",{attrs:{id:"class-notebooks"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#class-notebooks"}},[a._v("#")]),a._v(" Class Notebooks:")]),a._v(" "),e("p",[a._v("Here are links to our current Notebooks:")]),a._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://colab.research.google.com/github/derekphilipau/machinelearningforartists/blob/main/stylegan2_ada_pytorch_all_new5.ipynb",target:"_blank",rel:"noopener noreferrer"}},[a._v("Latest StyleGAN2 General Training Notebook (512px)"),e("OutboundLink")],1)]),a._v(" "),e("li",[e("a",{attrs:{href:"https://colab.research.google.com/github/derekphilipau/machinelearningforartists/blob/main/stylegan2_ada_pytorch_resume.ipynb",target:"_blank",rel:"noopener noreferrer"}},[a._v("StyleGAN2 Transfer Learning with Faces Notebook (512px)"),e("OutboundLink")],1)]),a._v(" "),e("li",[e("a",{attrs:{href:"https://colab.research.google.com/github/derekphilipau/machinelearningforartists/blob/main/stylegan2_ada_pytorch_resume_vases.ipynb",target:"_blank",rel:"noopener noreferrer"}},[a._v("StyleGAN2 Transfer Learning with Vases Notebook (1024px)"),e("OutboundLink")],1)])]),a._v(" "),e("h2",{attrs:{id:"various-stylegan2-links"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#various-stylegan2-links"}},[a._v("#")]),a._v(" Various StyleGAN2 links")]),a._v(" "),e("h3",{attrs:{id:"stylegan2-ada-official-pytorch-implementation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#stylegan2-ada-official-pytorch-implementation"}},[a._v("#")]),a._v(" StyleGAN2-ADA ‚Äî Official PyTorch implementation")]),a._v(" "),e("p",[a._v("https://github.com/NVlabs/stylegan2-ada-pytorch")]),a._v(" "),e("h3",{attrs:{id:"dvschultz-stylegan2-ada-pytorch"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dvschultz-stylegan2-ada-pytorch"}},[a._v("#")]),a._v(" dvschultz/stylegan2-ada-pytorch")]),a._v(" "),e("p",[a._v("new generators for stylegan2-ada-pytorch")]),a._v(" "),e("p",[a._v("https://github.com/dvschultz/stylegan2-ada-pytorch/blob/main/generate.py#L156")]),a._v(" "),e("p",[a._v("https://github.com/dvschultz/stylegan2-ada-pytorch")]),a._v(" "),e("p",[a._v("Colab\nhttps://github.com/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb")]),a._v(" "),e("h3",{attrs:{id:"stylegan2-surgery"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#stylegan2-surgery"}},[a._v("#")]),a._v(" StyleGAN2-Surgery")]),a._v(" "),e("p",[a._v("A collection of scripts and convenience modifications for creative media synthesis.")]),a._v(" "),e("p",[a._v("https://github.com/aydao/stylegan2-surgery")]),a._v(" "),e("h3",{attrs:{id:"pbaylies-projector-clip-py"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pbaylies-projector-clip-py"}},[a._v("#")]),a._v(" pbaylies/projector_clip.py")]),a._v(" "),e("p",[a._v("https://gist.github.com/pbaylies/671ef8434fd11f056bab4330e0e7c365")]),a._v(" "),e("h3",{attrs:{id:"pbaylies-stylegan2-ada"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pbaylies-stylegan2-ada"}},[a._v("#")]),a._v(" pbaylies/stylegan2-ada")]),a._v(" "),e("p",[a._v("https://github.com/pbaylies/stylegan2-ada")]),a._v(" "),e("h3",{attrs:{id:"closed-form-factorization-of-latent-semantics-in-gans"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#closed-form-factorization-of-latent-semantics-in-gans"}},[a._v("#")]),a._v(" Closed-Form Factorization of Latent Semantics in GANs")]),a._v(" "),e("p",[a._v("https://genforce.github.io/sefa/")]),a._v(" "),e("h3",{attrs:{id:"stylegan2-ada-operations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#stylegan2-ada-operations"}},[a._v("#")]),a._v(" StyleGAN2-ada operations")]),a._v(" "),e("p",[a._v("https://colab.research.google.com/drive/1kW8H8IkaFhoIVhf1uUhVoc93zZqllmg2?authuser=1#scrollTo=xzA1-mt88AO_")]),a._v(" "),e("h3",{attrs:{id:"encoding-in-style-a-stylegan-encoder-for-image-to-image-translation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#encoding-in-style-a-stylegan-encoder-for-image-to-image-translation"}},[a._v("#")]),a._v(" Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation")]),a._v(" "),e("p",[a._v("https://eladrich.github.io/pixel2style2pixel/")]),a._v(" "),e("h3",{attrs:{id:"stylegan2-ada-for-practice"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#stylegan2-ada-for-practice"}},[a._v("#")]),a._v(" StyleGAN2-ada for practice")]),a._v(" "),e("p",[a._v("This version of the newest PyTorch-based StyleGAN2-ada is intended mostly for fellow artists, who rarely look at scientific metrics, but rather need a working creative tool. Tested on Python 3.7 + PyTorch 1.7.1, requires FFMPEG for sequence-to-video conversions. For more explicit details refer to the original implementations.")]),a._v(" "),e("p",[a._v("https://github.com/eps696/stylegan2ada")]),a._v(" "),e("h3",{attrs:{id:"clip-glass"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#clip-glass"}},[a._v("#")]),a._v(" CLIP-GLaSS")]),a._v(" "),e("p",[a._v("https://colab.research.google.com/drive/1fWka_U56NhCegbbrQPt4PWpHPtNRdU49?usp=sharing#scrollTo=zvZFRZtcv8Mp")]),a._v(" "),e("h3",{attrs:{id:"text-guided-editing-of-images-using-clip-and-stylegan"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#text-guided-editing-of-images-using-clip-and-stylegan"}},[a._v("#")]),a._v(" Text-Guided Editing of Images (Using CLIP and StyleGAN)")]),a._v(" "),e("p",[a._v("https://github.com/orpatashnik/StyleCLIP")]),a._v(" "),e("h3",{attrs:{id:"navigating-stylegan2-w-latent-space-using-clip"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#navigating-stylegan2-w-latent-space-using-clip"}},[a._v("#")]),a._v(" Navigating StyleGAN2 w latent space using CLIP")]),a._v(" "),e("p",[a._v("https://github.com/l4rz/stylegan2-clip-approach")]),a._v(" "),e("h3",{attrs:{id:"navigating-stylegan2-w-latent-space-using-clip-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#navigating-stylegan2-w-latent-space-using-clip-2"}},[a._v("#")]),a._v(" Navigating StyleGAN2 ùëä latent space using CLIP")]),a._v(" "),e("p",[a._v("https://colab.research.google.com/drive/1IN3IgWQoB9WZGyz689COSGNnvnz7lIFI")]),a._v(" "),e("h3",{attrs:{id:"aphantasia"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#aphantasia"}},[a._v("#")]),a._v(" Aphantasia")]),a._v(" "),e("p",[a._v("https://github.com/eps696/aphantasia")])])}),[],!1,null,null,null);t.default=r.exports}}]);